{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Using Autograd.ipynb","provenance":[],"authorship_tag":"ABX9TyNUN2By6DRGG5EXdBlPjdkV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 자동 미분을 사용하여 신경망의 역전파 단계 연산을 자동화"],"metadata":{"id":"m4YsLLfehziI"}},{"cell_type":"markdown","source":[" Autograd를 사용하면, 신경망의 순전파 단계에서 연산 그래프(computational graph) 를 정의하게 된다. 이 그래프의 노드(node)는 텐서(tensor)이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를 만들어내는 함수. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있음."],"metadata":{"id":"vLBWaDNKiBO5"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"26BGCXzhhw7X","executionInfo":{"status":"ok","timestamp":1653118471277,"user_tz":-540,"elapsed":3925,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}}},"outputs":[],"source":["import torch\n","import math\n","\n","dtype = torch.float\n","device = torch.device(\"cpu\")\n","# device = torch.device(\"cuda:0\") -- GPU를 사용할 경우"]},{"cell_type":"code","source":["# 입력과 출력값 텐서를 생성\n","# requires_grad = False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가 없음을 나타냄\n","\n","x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n","y = torch.sin(x)\n","\n","print(x.shape[0])\n","print(y.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bYjfsZ9ieUG","executionInfo":{"status":"ok","timestamp":1653118583425,"user_tz":-540,"elapsed":6,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"90a76380-ac4f-4540-f1e1-0ab78879e7f7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2000\n","2000\n"]}]},{"cell_type":"code","source":["# 가중치 텐서 생성 3차 다항식이므로 4개의 가중치 필요\n","# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서에 대한 변화도를 계산할 수 있도록 함.\n","\n","a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n","d = torch.randn((), device=device, dtype=dtype, requires_grad=True)"],"metadata":{"id":"RrW7yFDJi8CL","executionInfo":{"status":"ok","timestamp":1653118684119,"user_tz":-540,"elapsed":275,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["learning_rate = 1e-6\n","num_iteration = 5000\n","for t in range(num_iteration) :\n","  # 순전파 단계\n","  y_pred = a + b * x + c * x ** 2 + d * x ** 3\n","\n","  # loss는 (1,) shape을 갖는 텐서입니다.\n","  # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있습니다.\n","  loss = (y_pred - y).pow(2).sum()\n","  if t % 1000 == 999:\n","    print(t, loss.item())\n","\n","  # autograd 를 사용하여 역전파 단계를 계산합니다. \n","  # 이는 requires_grad=True를 갖는 모든 텐서들에 대한 손실의 변화도를 계산\n","  # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 loss의 변화도를 갖는 텐서가 된다\n","  loss.backward()\n","\n","  # 경사하강법(gradient descent)를 사용하여 가중치를 직접 갱신\n","  # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만 autograd에서 이를 추적하지 않아도 되기 때문\n","  with torch.no_grad():\n","    a -= learning_rate * a.grad\n","    b -= learning_rate * b.grad\n","    c -= learning_rate * c.grad\n","    d -= learning_rate * d.grad\n","\n","    # 가중치 갱신 후에는 변화도를 직접 0으로 만든다 : gradient를 더하는 식으로 연산이 되기 때문\n","    a.grad = None\n","    b.grad = None\n","    c.grad = None\n","    d.grad = None\n","\n","print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUAJdiWZjUhN","executionInfo":{"status":"ok","timestamp":1653119522918,"user_tz":-540,"elapsed":1282,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"ac2eee9b-d574-4600-a010-7ea55a48156c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["999 8.817167282104492\n","1999 8.817167282104492\n","2999 8.817167282104492\n","3999 8.817167282104492\n","4999 8.817167282104492\n","Result: y = 7.450554395660447e-09 + 0.8567265868186951 x + -1.1153121981521963e-08 x^2 + -0.09332836419343948 x^3\n"]}]},{"cell_type":"markdown","source":["###python with문\n","자원을 획득하고 사용 후 반납해야 하는 경우 주로 사용\n","\n","1) 자원을 획득한다\n","\n","2) 자원을 사용한다\n","\n","3) 자원을 반납한다\n","\n","​"],"metadata":{"id":"--SjzvT1mxP4"}}]}