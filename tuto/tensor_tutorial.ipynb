{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7Xnj5bxNkHKV"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"r63rOPUKkHKX"},"source":["\n","`Learn the Basics <intro.html>`_ ||\n","`Quickstart <quickstart_tutorial.html>`_ || \n","**Tensors** || \n","`Datasets & DataLoaders <data_tutorial.html>`_ ||\n","`Transforms <transforms_tutorial.html>`_ ||\n","`Build Model <buildmodel_tutorial.html>`_ ||\n","`Autograd <autograd_tutorial.html>`_ ||\n","`Optimization <optimization_tutorial.html>`_ ||\n","`Save & Load Model <saveloadrun_tutorial.html>`_\n","\n","Tensors \n","==========================\n","\n","Tensors are a specialized data structure that are very similar to arrays and matrices. \n","In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n","\n","Tensors are similar to `NumPy’s <https://numpy.org/>`_ ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\n","NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `bridge-to-np-label`). Tensors \n","are also optimized for automatic differentiation (we'll see more about that later in the `Autograd <autograd_tutorial.html>`__ \n","section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQeusaSLkHKZ"},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"DbvPtyyWkHKa"},"source":["Initializing a Tensor\n","~~~~~~~~~~~~~~~~~~~~~\n","\n","Tensors can be initialized in various ways. Take a look at the following examples:\n","\n","**Directly from data**\n","\n","Tensors can be created directly from data. The data type is automatically inferred.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBLggOZfkHKa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417137267,"user_tz":-540,"elapsed":265,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"7683b9ab-1023-490a-9d24-a8711f19547e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2],\n","        [3, 4]])\n"]}],"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)\n","print(x_data)"]},{"cell_type":"markdown","metadata":{"id":"8UpYhDnukHKa"},"source":["**From a NumPy array**\n","\n","Tensors can be created from NumPy arrays (and vice versa - see `bridge-to-np-label`).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPnw1hQ7kHKc"},"outputs":[],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"]},{"cell_type":"markdown","metadata":{"id":"GtDlrKa3kHKc"},"source":["**From another tensor:**\n","\n","The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baAPXmfYkHKc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417141876,"user_tz":-540,"elapsed":3,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"6b9094ea-a885-4579-ebdf-a2ff4274632c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.5195, 0.1122],\n","        [0.4203, 0.6369]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"buc5Val8kHKd"},"source":["**With random or constant values:**\n","\n","``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAWpv9ckkHKd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417143711,"user_tz":-540,"elapsed":3,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"b0b4734e-e889-44b9-c092-329da098398c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.4662, 0.5416, 0.0023],\n","        [0.5086, 0.9055, 0.9981]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"kIew3s6ckHKd"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NZK6lhydkHKd"},"source":["Attributes of a Tensor\n","~~~~~~~~~~~~~~~~~\n","\n","Tensor attributes describe their shape, datatype, and the device on which they are stored.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1LZOQ4skHKe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417145384,"user_tz":-540,"elapsed":3,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"b7b7df78-61b3-44e8-a28b-e92f1bca8b81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"n_tx5P6kkHKe"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"10_eTZEikHKe"},"source":["Operations on Tensors\n","~~~~~~~~~~~~~~~~~\n","\n","Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, \n","indexing, slicing), sampling and more are\n","comprehensively described `here <https://pytorch.org/docs/stable/torch.html>`__.\n","\n","Each of these operations can be run on the GPU (at typically higher speeds than on a\n","CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n","\n","By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using \n","``.to`` method (after checking for GPU availability). Keep in mind that copying large tensors\n","across devices can be expensive in terms of time and memory!\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wa2a6d0pkHKf"},"outputs":[],"source":["# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","  tensor = tensor.to('cuda')"]},{"cell_type":"code","source":["import torch\n","device = torch.device('cpu')\n","torch.cuda.set_device(sw)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"UawEdEjU-l_a","executionInfo":{"status":"error","timestamp":1656045155043,"user_tz":-540,"elapsed":289,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"fa749637-bac6-4fc3-8871-74ce31b2caa2"},"execution_count":2,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e729fbad7391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: set_device() missing 1 required positional argument: 'device'"]}]},{"cell_type":"markdown","metadata":{"id":"JiM9AxHlkHKf"},"source":["Try out some of the operations from the list.\n","If you're familiar with the NumPy API, you'll find the Tensor API a breeze to use.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2rOQQODqkHKf"},"source":["**Standard numpy-like indexing and slicing:**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYIwzgLqkHKf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417285794,"user_tz":-540,"elapsed":237,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"19b56357-b00e-420d-b5d1-af35b7122551"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row:  tensor([1., 1., 1., 1.])\n","First column:  tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","print('First row: ',tensor[0])\n","print('First column: ', tensor[:, 0])\n","print('Last column:', tensor[..., -1])\n","tensor[:,1] = 0\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"jtS2NoY2kHKf"},"source":["**Joining tensors** You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n","See also `torch.stack <https://pytorch.org/docs/stable/generated/torch.stack.html>`__,\n","another tensor joining op that is subtly different from ``torch.cat``.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47DSLvoTkHKg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652417429641,"user_tz":-540,"elapsed":246,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"dd127067-e07b-4358-aa66-05158aeec5bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"]}],"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"]},{"cell_type":"markdown","metadata":{"id":"1DClgH6pkHKg"},"source":["**Arithmetic operations**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVlFXIALkHKg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652418585636,"user_tz":-540,"elapsed":251,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"aa038197-9b54-4abd-cf1e-729deab71167"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])"]},"metadata":{},"execution_count":13}],"source":["# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(tensor)\n","torch.matmul(tensor, tensor.T, out=y3)\n","\n","\n","# This computes the element-wise product. z1, z2, z3 will have the same value\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","\n","z3 = torch.rand_like(tensor)\n","torch.mul(tensor, tensor, out=z3)"]},{"cell_type":"markdown","metadata":{"id":"SyR-yRFHkHKg"},"source":["**Single-element tensors** If you have a one-element tensor, for example by aggregating all\n","values of a tensor into one value, you can convert it to a Python\n","numerical value using ``item()``:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9giybDykHKg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652419783115,"user_tz":-540,"elapsed":233,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"9eef8fa9-0d9d-41e8-b2e7-7835d10b03bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(92.)\n","92.0 <class 'float'>\n"]}],"source":["agg = tensor.sum()\n","agg_item = agg.item()  \n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","metadata":{"id":"F76mVjIhkHKh"},"source":["**In-place operations**\n","Operations that store the result into the operand are called in-place. They are denoted by a ``_`` suffix. \n","For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce2_Meb8kHKh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652420141988,"user_tz":-540,"elapsed":210,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"058b051f-565f-4194-de67-6097f7a5c15f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]]) \n","\n","tensor([[6., 6., 6., 6.],\n","        [6., 6., 6., 6.],\n","        [6., 6., 6., 6.],\n","        [6., 6., 6., 6.]]) \n","\n","tensor([[6., 6., 6., 6.],\n","        [6., 6., 6., 6.],\n","        [6., 6., 6., 6.],\n","        [6., 6., 6., 6.]]) \n","\n","tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])\n"]}],"source":["print(tensor, \"\\n\")\n","a = torch.ones_like(tensor)\n","tensor.add_(5)\n","print(tensor, \"\\n\")\n","print(tensor.t_(), \"\\n\")\n","print(tensor.copy_(a))"]},{"cell_type":"markdown","metadata":{"id":"EXRP_c_YkHKh"},"source":["<div class=\"alert alert-info\"><h4>Note</h4><p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\n","     of history. Hence, their use is discouraged.</p></div>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Th5-0CEHkHKh"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XZBf6N7QkHKh"},"source":["\n","Bridge with NumPy\n","~~~~~~~~~~~~~~~~~\n","Tensors on the CPU and NumPy arrays can share their underlying memory\n","locations, and changing one will change\tthe other.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0mvnqZJLkHKi"},"source":["Tensor to NumPy array\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZndpNBKkHKi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652420286064,"user_tz":-540,"elapsed":218,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"3fe6a8c4-4df5-4e78-cd77-bf84c97e0ccc"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([1., 1., 1., 1., 1.])\n","n: [1. 1. 1. 1. 1.]\n"]}],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"QxM5Y8qXkHKi"},"source":["A change in the tensor reflects in the NumPy array.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EH0Ts3LwkHKi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652420259538,"user_tz":-540,"elapsed":225,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"5b046388-5b27-439f-bbe1-a7965014f493"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.])\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"yzvepaj_kHKi"},"source":["NumPy array to Tensor\n","^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wDbY84skHKi"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"5ec2-M1vkHKj"},"source":["Changes in the NumPy array reflects in the tensor.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3NG3iRfkHKj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652420263357,"user_tz":-540,"elapsed":229,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"9af732a4-31ed-4145-fb34-07f39f21488c"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"tensor_tutorial.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/79fb1f271c0c8c0a68a8bceda380d03d/tensor_tutorial.ipynb","timestamp":1652414270557}]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}