{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN  using nn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPMbLS6HmQPBSgtNTFIvoPF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Build CNN\n","Used Pytorch's Conv2d class as Convolutional layers.\n","Each convolution is followed by a ReLU and perform an average pooling at the end."],"metadata":{"id":"Drjy7F3D3m8A"}},{"cell_type":"code","source":["# load data\n","from pathlib import Path\n","import requests\n","\n","DATA_PATH = Path(\"data\")\n","PATH = DATA_PATH / \"mnist\"\n","\n","PATH.mkdir(parents=True, exist_ok=True)\n","\n","URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n","FILENAME = \"mnist.pkl.gz\"\n","\n","if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)"],"metadata":{"id":"0hrCGLdOVDc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import gzip\n","\n","with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"],"metadata":{"id":"UWDjZssAVFF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import math\n","import torch.nn.functional as F\n","from torch import optim\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","import numpy as np"],"metadata":{"id":"q4jq0r8J4JlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Mnist_CNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n","    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","    self.conv3 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n","\n","  def forward(self, xb):\n","    xb = xb.view(-1, 1, 28, 28)\n","    xb = F.relu(self.conv1(xb))\n","    xb = F.relu(self.conv2(xb))\n","    xb = F.relu(self.conv3(xb))\n","    xb = F.avg_pool2d(xb, 4)\n","    return xb.view(-1, xb.size(1))"],"metadata":{"id":"L9Jv5Cs13mFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIAzLkHv3iB0"},"outputs":[],"source":["model = Mnist_CNN()"]},{"cell_type":"code","source":["def loss_batch(model, loss_func, xb, yb, opt=None):\n","    loss = loss_func(model(xb), yb)\n","\n","    if opt is not None:\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","\n","    return loss.item(), len(xb)\n","\n","def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    for epoch in range(epochs):\n","        model.train()\n","        for xb, yb in train_dl:\n","            loss_batch(model, loss_func, xb, yb, opt)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            losses, nums = zip(\n","                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n","            )\n","        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n","\n","        print(epoch, val_loss)"],"metadata":{"id":"JtNdhhZ3Uo4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train, x_valid, y_valid = map(\n","    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",")\n","n, c = x_train.shape\n","print(x_train, y_train)\n","print(x_train.shape)\n","print(y_train.min(), y_train.max())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hYH9aEXcWCg4","executionInfo":{"status":"ok","timestamp":1655967330548,"user_tz":-540,"elapsed":274,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"f95ccb5b-1aa5-466f-a8d4-c6dc731d0483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n","torch.Size([50000, 784])\n","tensor(0) tensor(9)\n"]}]},{"cell_type":"code","source":["bs = 64\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","\n","valid_ds = TensorDataset(x_valid, y_valid)\n","valid_dl = DataLoader(valid_ds, batch_size=bs * 2)"],"metadata":{"id":"ljWa7xZPUyKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = 0.1\n","epochs = 3\n","loss_func = F.cross_entropy\n","opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n","fit(epochs, model, loss_func, opt, train_dl, valid_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipraNvsjUVYx","executionInfo":{"status":"ok","timestamp":1655967417012,"user_tz":-540,"elapsed":17859,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"c138900c-c1f2-4687-8715-b32d0dbe93ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.4711491441965103\n","1 0.26767015171051023\n","2 0.21798026893138886\n"]}]},{"cell_type":"markdown","source":["###nn.Sequentce 사용"],"metadata":{"id":"IqptsKhL_QZf"}},{"cell_type":"code","source":[""],"metadata":{"id":"-w8d8WYd_TqU"},"execution_count":null,"outputs":[]}]}