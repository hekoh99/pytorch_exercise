{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"WmWEVjlfRVng","executionInfo":{"status":"ok","timestamp":1652588529944,"user_tz":-540,"elapsed":3,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"zWTatwasRVnj"},"source":["\n","`Learn the Basics <intro.html>`_ ||\n","`Quickstart <quickstart_tutorial.html>`_ ||\n","`Tensors <tensorqs_tutorial.html>`_ ||\n","`Datasets & DataLoaders <data_tutorial.html>`_ ||\n","`Transforms <transforms_tutorial.html>`_ ||\n","**Build Model** ||\n","`Autograd <autogradqs_tutorial.html>`_ ||\n","`Optimization <optimization_tutorial.html>`_ ||\n","`Save & Load Model <saveloadrun_tutorial.html>`_\n","\n","Build the Neural Network\n","===================\n","\n","Neural networks comprise of layers/modules that perform operations on data.\n","The `torch.nn <https://pytorch.org/docs/stable/nn.html>`_ namespace provides all the building blocks you need to\n","build your own neural network. Every module in PyTorch subclasses the `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_.\n","A neural network is a module itself that consists of other modules (layers). This nested structure allows for\n","building and managing complex architectures easily.\n","\n","In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XtYAsCY_RVnl","executionInfo":{"status":"ok","timestamp":1652588536068,"user_tz":-540,"elapsed":3468,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}}},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"tLEws0sVRVnl"},"source":["Get Device for Training\n","-----------------------\n","We want to be able to train our model on a hardware accelerator like the GPU,\n","if it is available. Let's check to see if\n","`torch.cuda <https://pytorch.org/docs/stable/notes/cuda.html>`_ is available, else we\n","continue to use the CPU.\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8KW2P2RPRVnm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652588538341,"user_tz":-540,"elapsed":281,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"4314197d-1ab1-4703-c34a-c3418c9db611"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"OUMs2ucoRVnm"},"source":["Define the Class\n","-------------------------\n","We define our neural network by subclassing ``nn.Module``, and\n","initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements\n","the operations on input data in the ``forward`` method.\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"f46rcByRRVnm","executionInfo":{"status":"ok","timestamp":1652588542896,"user_tz":-540,"elapsed":306,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}}},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"qkUbjA1sRVnn"},"source":["We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print\n","its structure.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddfLK7fZRVnn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652511708099,"user_tz":-540,"elapsed":11339,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"c7cd0d78-a6c4-4a5b-cbe1-63708f8e4b13"},"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["model = NeuralNetwork().to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"Zt-x9Bp1RVnn"},"source":["To use the model, we pass it the input data. This executes the model's ``forward``,\n","along with some `background operations <https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866>`_.\n","Do not call ``model.forward()`` directly!\n","\n","Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class.\n","We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVFcr5QCRVno","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512358437,"user_tz":-540,"elapsed":239,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"43573551-574a-4381-9bd2-9ec679759c8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([2], device='cuda:0')\n"]}],"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{"id":"ytQU8eUpRVno"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gURtHFphRVno"},"source":["Model Layers\n","-------------------------\n","\n","Let's break down the layers in the FashionMNIST model. To illustrate it, we\n","will take a sample minibatch of 3 images of size 28x28 and see what happens to it as\n","we pass it through the network.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cg170DvRRVnp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512883961,"user_tz":-540,"elapsed":258,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"1a36e6cd-2887-414c-9e23-de80bbdf6ad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}],"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"]},{"cell_type":"markdown","metadata":{"id":"vGv_IqASRVnp"},"source":["nn.Flatten\n","^^^^^^^^^^^^^^^^^^^^^^\n","We initialize the `nn.Flatten  <https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html>`_\n","layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\n","the minibatch dimension (at dim=0) is maintained).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMeELYU2RVnp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512885719,"user_tz":-540,"elapsed":2,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"50ee4ad5-8ed7-48fa-dd5d-c545e62ffb2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}],"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"]},{"cell_type":"markdown","metadata":{"id":"fMi16P-4RVnp"},"source":["nn.Linear\n","^^^^^^^^^^^^^^^^^^^^^^\n","The `linear layer <https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>`_\n","is a module that applies a linear transformation on the input using its stored weights and biases.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZRBli4iRVnq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512886913,"user_tz":-540,"elapsed":394,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"9b7c1641-cacb-4c2a-def1-0a48150ea979"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}],"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"jhhXAUEvRVnq"},"source":["nn.ReLU\n","^^^^^^^^^^^^^^^^^^^^^^\n","Non-linear activations are what create the complex mappings between the model's inputs and outputs.\n","They are applied after linear transformations to introduce *nonlinearity*, helping neural networks\n","learn a wide variety of phenomena.\n","\n","In this model, we use `nn.ReLU <https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html>`_ between our\n","linear layers, but there's other activations to introduce non-linearity in your model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMWrrcZJRVnq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512888198,"user_tz":-540,"elapsed":389,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"6d40e6d6-1745-4a1e-a5b7-a551c7a4d85f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[-0.2528, -0.0268,  0.0114, -0.0045,  0.3784,  0.2001,  0.0447,  0.2714,\n","         -0.7444, -0.3409,  0.9983,  0.1617,  0.1768, -0.5889,  0.3005,  0.0693,\n","          0.0446,  0.2704,  0.4592,  0.1773],\n","        [ 0.2742, -0.2606,  0.1931, -0.2226,  0.4267, -0.0194,  0.1990,  0.5421,\n","         -0.4679, -0.2574,  0.7433,  0.1376,  0.1080, -0.1894,  0.2660,  0.5061,\n","          0.2967,  0.3765,  0.1181,  0.2137],\n","        [-0.1166, -0.1231, -0.1971,  0.1231, -0.0314,  0.2382,  0.3909,  0.3680,\n","         -0.3884, -0.2871,  0.9466,  0.1100,  0.3947, -0.2848,  0.1792,  0.1722,\n","          0.1850,  0.0045,  0.1553, -0.0257]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.0000, 0.0000, 0.0114, 0.0000, 0.3784, 0.2001, 0.0447, 0.2714, 0.0000,\n","         0.0000, 0.9983, 0.1617, 0.1768, 0.0000, 0.3005, 0.0693, 0.0446, 0.2704,\n","         0.4592, 0.1773],\n","        [0.2742, 0.0000, 0.1931, 0.0000, 0.4267, 0.0000, 0.1990, 0.5421, 0.0000,\n","         0.0000, 0.7433, 0.1376, 0.1080, 0.0000, 0.2660, 0.5061, 0.2967, 0.3765,\n","         0.1181, 0.2137],\n","        [0.0000, 0.0000, 0.0000, 0.1231, 0.0000, 0.2382, 0.3909, 0.3680, 0.0000,\n","         0.0000, 0.9466, 0.1100, 0.3947, 0.0000, 0.1792, 0.1722, 0.1850, 0.0045,\n","         0.1553, 0.0000]], grad_fn=<ReluBackward0>)\n"]}],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"vv6VtKk7RVnq"},"source":["nn.Sequential\n","^^^^^^^^^^^^^^^^^^^^^^\n","`nn.Sequential <https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>`_ is an ordered\n","container of modules. The data is passed through all the modules in the same order as defined. You can use\n","sequential containers to put together a quick network like ``seq_modules``.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMqudacrRVnr"},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"]},{"cell_type":"markdown","metadata":{"id":"FXDoP7RbRVnr"},"source":["nn.Softmax\n","^^^^^^^^^^^^^^^^^^^^^^\n","The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the\n","`nn.Softmax <https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html>`_ module. The logits are scaled to values\n","[0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along\n","which the values must sum to 1.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1cfUMY3RVnr"},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"]},{"cell_type":"markdown","metadata":{"id":"f3QK-YmLRVnr"},"source":["Model Parameters\n","-------------------------\n","Many layers inside a neural network are *parameterized*, i.e. have associated weights\n","and biases that are optimized during training. Subclassing ``nn.Module`` automatically\n","tracks all fields defined inside your model object, and makes all parameters\n","accessible using your model's ``parameters()`` or ``named_parameters()`` methods.\n","\n","In this example, we iterate over each parameter, and print its size and a preview of its values.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qvd8SzMlRVnr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652512892324,"user_tz":-540,"elapsed":3,"user":{"displayName":"Haeun Ko","userId":"00138316170718484361"}},"outputId":"fc679765-b801-473c-e655-1e151d08e3e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0226, -0.0262, -0.0111,  ...,  0.0082,  0.0203, -0.0105],\n","        [-0.0115,  0.0114,  0.0042,  ...,  0.0095, -0.0074, -0.0018]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0239, -0.0083], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0357, -0.0398,  0.0284,  ...,  0.0111, -0.0300, -0.0186],\n","        [-0.0408, -0.0336, -0.0267,  ..., -0.0009, -0.0072,  0.0393]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0350,  0.0182], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0238, -0.0065, -0.0345,  ...,  0.0147,  0.0392, -0.0078],\n","        [ 0.0155, -0.0037,  0.0029,  ..., -0.0123,  0.0068, -0.0189]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0204,  0.0061], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n"]}],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"WNNWeF_sRVns"},"source":["--------------\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"91uMLvcpRVns"},"source":["Further Reading\n","--------------\n","- `torch.nn API <https://pytorch.org/docs/stable/nn.html>`_\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"name":"buildmodel_tutorial.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/ea0953da74a725b00248ac3fceada0f6/buildmodel_tutorial.ipynb","timestamp":1652509995864}]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}